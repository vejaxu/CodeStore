{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec1f0ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdd7029a3d74e86a304288b46d63866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "model_path = \"/home/xwj/Model/llama2-7b-hf\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "texts = [\n",
    "    \"the quick brown fox\" , \n",
    "    \"the lazy dog sleeps\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dcbd71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,   278,  4996, 17354,  1701, 29916],\n",
      "        [    1,   278, 17366, 11203, 12844,  8961]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([2, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/xwj_llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "encoded_texts = tokenizer(texts, return_tensors=\"pt\", padding=True, max_length=128)\n",
    "print(encoded_texts)\n",
    "print(encoded_texts[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f38563df",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encoded_texts[\"input_ids\"]\n",
    "attention_mask = encoded_texts[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5583c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalLMOutputWithPast(loss=None, logits=tensor([[[-12.9832,  -7.4134,  -0.4327,  ...,  -6.8297,  -8.0880,  -7.5863],\n",
      "         [-14.1865, -12.8350,  -7.1458,  ...,  -9.7179, -10.8815,  -9.9526],\n",
      "         [-11.4141, -13.2432,  -2.1016,  ...,  -7.7475,  -8.8215,  -7.6209],\n",
      "         [ -9.7292,  -6.5780,   5.4661,  ...,  -4.5612,  -5.7276,  -5.6714],\n",
      "         [ -7.4323,  -7.9920,   9.5942,  ...,  -0.8915,  -2.7065,  -0.3414],\n",
      "         [-11.2905,  -9.2060,   2.6828,  ...,  -5.9094,  -7.0724,  -5.8816]],\n",
      "\n",
      "        [[-12.9832,  -7.4134,  -0.4327,  ...,  -6.8297,  -8.0880,  -7.5863],\n",
      "         [-14.1865, -12.8350,  -7.1458,  ...,  -9.7179, -10.8815,  -9.9526],\n",
      "         [ -9.3224,  -7.6790,   0.1042,  ...,  -8.0256,  -6.5289,  -7.0641],\n",
      "         [-13.4260, -13.3056,   1.9306,  ...,  -7.2037,  -8.3358, -10.3643],\n",
      "         [ -4.3707,  -2.0274,   7.6155,  ...,   1.2817,  -0.4330,   0.7789],\n",
      "         [ -7.7859,  -7.8256,   8.4216,  ...,  -3.0686,  -5.1709,  -6.4460]]]), past_key_values=<transformers.cache_utils.DynamicCache object at 0x7c906d75ead0>, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "print(outputs)\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e060248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[4.3589e-10, 1.1438e-07, 1.2302e-04,  ..., 2.0504e-07,\n",
      "          5.8260e-08, 9.6210e-08],\n",
      "         [2.0143e-09, 7.7822e-09, 2.3008e-06,  ..., 1.7573e-07,\n",
      "          5.4891e-08, 1.3896e-07],\n",
      "         [5.3176e-10, 8.5377e-11, 5.8896e-06,  ..., 2.0801e-08,\n",
      "          7.1065e-09, 2.3608e-08],\n",
      "         [7.4380e-11, 1.7379e-09, 2.9559e-04,  ..., 1.3059e-08,\n",
      "          4.0676e-09, 4.3027e-09],\n",
      "         [2.1137e-13, 1.2077e-13, 5.2427e-06,  ..., 1.4645e-10,\n",
      "          2.3847e-11, 2.5384e-10]],\n",
      "\n",
      "        [[4.3589e-10, 1.1438e-07, 1.2302e-04,  ..., 2.0504e-07,\n",
      "          5.8260e-08, 9.6210e-08],\n",
      "         [2.0144e-09, 7.7821e-09, 2.3008e-06,  ..., 1.7573e-07,\n",
      "          5.4891e-08, 1.3896e-07],\n",
      "         [1.0697e-09, 5.5328e-09, 1.3278e-05,  ..., 3.9125e-09,\n",
      "          1.7476e-08, 1.0233e-08],\n",
      "         [1.4699e-10, 1.6580e-10, 6.8635e-04,  ..., 7.4057e-08,\n",
      "          2.3874e-08, 3.1402e-09],\n",
      "         [4.6585e-12, 4.8524e-11, 7.4784e-07,  ..., 1.3276e-09,\n",
      "          2.3899e-10, 8.0300e-10]]])\n"
     ]
    }
   ],
   "source": [
    "next_token_logits = logits[:, : -1, :]\n",
    "target_ids = input_ids[:, 1: ]\n",
    "probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d8dbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "原句：the quick brown fox\n",
      "位置 0 输入token: '<s>'\n",
      "        模型预测: '#'，真实下一个: 'the'\n",
      "\n",
      "位置 1 输入token: 'the'\n",
      "        模型预测: ''，真实下一个: 'quick'\n",
      "\n",
      "位置 2 输入token: 'quick'\n",
      "        模型预测: 'est'，真实下一个: 'brown'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_sentence = texts[0]\n",
    "print(f\"\\n原句: {first_sentence}\")\n",
    "for i in range(min(3, input_ids.shape[1] - 1)):\n",
    "    input_token = tokenizer.decode([input_ids[0, i]])\n",
    "    target_token = tokenizer.decode([target_ids[0, i]])\n",
    "    predicted_token_id = torch.argmax(probs[0, i]).item()\n",
    "    predicted_token = tokenizer.decode([predicted_token_id])\n",
    "    \n",
    "    print(f\"位置 {i} 输入token: '{input_token}'\")\n",
    "    print(f\"        模型预测: '{predicted_token}'，真实下一个: '{target_token}'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb86acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xwj_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
