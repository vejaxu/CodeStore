{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37bbad02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1024])\n",
      "tensor([[0.7646, 0.1414],\n",
      "        [0.1355, 0.6000]], grad_fn=<MmBackward0>)\n",
      "[[0.7645566463470459, 0.14142508804798126], [0.13549773395061493, 0.5999549627304077]]\n"
     ]
    }
   ],
   "source": [
    "# Requires transformers>=4.51.0\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# 取最后一个token的embedding作为整个句子的embedding\n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery:{query}'\n",
    "\n",
    "\n",
    "def tokenize(tokenizer, input_texts, eod_id, max_length):\n",
    "    batch_dict = tokenizer(input_texts, padding=False, truncation=True, max_length=max_length-2)\n",
    "    for seq, att in zip(batch_dict[\"input_ids\"], batch_dict[\"attention_mask\"]):\n",
    "        seq.append(eod_id)\n",
    "        att.append(1)\n",
    "    batch_dict = tokenizer.pad(batch_dict, padding=True, return_tensors=\"pt\")\n",
    "    return batch_dict\n",
    "\n",
    "\n",
    "# Each query must come with a one-sentence instruction that describes the task\n",
    "task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "\n",
    "queries = [\n",
    "    get_detailed_instruct(task, 'What is the capital of China?'),\n",
    "    get_detailed_instruct(task, 'Explain gravity')\n",
    "]\n",
    "# No need to add instruction for retrieval documents\n",
    "documents = [\n",
    "    \"The capital of China is Beijing.\",\n",
    "    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n",
    "]\n",
    "\n",
    "input_texts = queries + documents\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('/home/xwj/Model/qwen3-embedding-0.6b', padding_side='left')\n",
    "model = AutoModel.from_pretrained('/home/xwj/Model/qwen3-embedding-0.6b')\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n",
    "# model = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-0.6B', attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16).cuda()\n",
    "\n",
    "eod_id = tokenizer.convert_tokens_to_ids(\"<|endoftext|>\")\n",
    "max_length = 8192\n",
    "\n",
    "# Tokenize the input texts\n",
    "batch_dict = tokenize(tokenizer, input_texts, eod_id, max_length)\n",
    "batch_dict.to(model.device)\n",
    "outputs = model(**batch_dict)\n",
    "embeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "print(embeddings.shape)\n",
    "\n",
    "# normalize embeddings\n",
    "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "scores = (embeddings[:2] @ embeddings[2:].T)\n",
    "print(scores)\n",
    "print(scores.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19977b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Qwen3Model(\n",
      "  (embed_tokens): Embedding(151669, 1024)\n",
      "  (layers): ModuleList(\n",
      "    (0-27): 28 x Qwen3DecoderLayer(\n",
      "      (self_attn): Qwen3Attention(\n",
      "        (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "        (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "      )\n",
      "      (mlp): Qwen3MLP(\n",
      "        (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "      (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (rotary_emb): Qwen3RotaryEmbedding()\n",
      ")\n",
      "embed_tokens Embedding(151669, 1024)\n",
      "layers ModuleList(\n",
      "  (0-27): 28 x Qwen3DecoderLayer(\n",
      "    (self_attn): Qwen3Attention(\n",
      "      (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "      (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "      (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "      (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    )\n",
      "    (mlp): Qwen3MLP(\n",
      "      (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "      (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "      (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "    (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  )\n",
      ")\n",
      "layers.0 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.0.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.0.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.0.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.0.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.0.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.0.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.0.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.0.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.0.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.0.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.0.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.0.mlp.act_fn SiLU()\n",
      "layers.0.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.0.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.1 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.1.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.1.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.1.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.1.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.1.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.1.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.1.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.1.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.1.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.1.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.1.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.1.mlp.act_fn SiLU()\n",
      "layers.1.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.1.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.2 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.2.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.2.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.2.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.2.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.2.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.2.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.2.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.2.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.2.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.2.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.2.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.2.mlp.act_fn SiLU()\n",
      "layers.2.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.2.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.3 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.3.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.3.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.3.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.3.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.3.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.3.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.3.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.3.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.3.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.3.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.3.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.3.mlp.act_fn SiLU()\n",
      "layers.3.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.3.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.4 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.4.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.4.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.4.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.4.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.4.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.4.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.4.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.4.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.4.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.4.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.4.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.4.mlp.act_fn SiLU()\n",
      "layers.4.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.4.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.5 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.5.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.5.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.5.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.5.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.5.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.5.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.5.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.5.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.5.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.5.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.5.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.5.mlp.act_fn SiLU()\n",
      "layers.5.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.5.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.6 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.6.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.6.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.6.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.6.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.6.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.6.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.6.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.6.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.6.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.6.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.6.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.6.mlp.act_fn SiLU()\n",
      "layers.6.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.6.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.7 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.7.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.7.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.7.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.7.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.7.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.7.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.7.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.7.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.7.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.7.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.7.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.7.mlp.act_fn SiLU()\n",
      "layers.7.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.7.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.8 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.8.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.8.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.8.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.8.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.8.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.8.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.8.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.8.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.8.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.8.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.8.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.8.mlp.act_fn SiLU()\n",
      "layers.8.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.8.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.9 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.9.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.9.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.9.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.9.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.9.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.9.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.9.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.9.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.9.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.9.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.9.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.9.mlp.act_fn SiLU()\n",
      "layers.9.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.9.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.10 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.10.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.10.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.10.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.10.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.10.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.10.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.10.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.10.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.10.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.10.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.10.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.10.mlp.act_fn SiLU()\n",
      "layers.10.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.10.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.11 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.11.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.11.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.11.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.11.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.11.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.11.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.11.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.11.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.11.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.11.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.11.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.11.mlp.act_fn SiLU()\n",
      "layers.11.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.11.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.12 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.12.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.12.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.12.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.12.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.12.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.12.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.12.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.12.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.12.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.12.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.12.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.12.mlp.act_fn SiLU()\n",
      "layers.12.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.12.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.13 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.13.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.13.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.13.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.13.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.13.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.13.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.13.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.13.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.13.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.13.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.13.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.13.mlp.act_fn SiLU()\n",
      "layers.13.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.13.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.14 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.14.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.14.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.14.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.14.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.14.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.14.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.14.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.14.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.14.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.14.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.14.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.14.mlp.act_fn SiLU()\n",
      "layers.14.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.14.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.15 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.15.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.15.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.15.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.15.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.15.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.15.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.15.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.15.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.15.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.15.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.15.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.15.mlp.act_fn SiLU()\n",
      "layers.15.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.15.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.16 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.16.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.16.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.16.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.16.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.16.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.16.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.16.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.16.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.16.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.16.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.16.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.16.mlp.act_fn SiLU()\n",
      "layers.16.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.16.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.17 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.17.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.17.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.17.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.17.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.17.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.17.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.17.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.17.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.17.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.17.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.17.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.17.mlp.act_fn SiLU()\n",
      "layers.17.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.17.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.18 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.18.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.18.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.18.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.18.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.18.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.18.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.18.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.18.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.18.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.18.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.18.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.18.mlp.act_fn SiLU()\n",
      "layers.18.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.18.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.19 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.19.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.19.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.19.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.19.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.19.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.19.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.19.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.19.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.19.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.19.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.19.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.19.mlp.act_fn SiLU()\n",
      "layers.19.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.19.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.20 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.20.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.20.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.20.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.20.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.20.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.20.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.20.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.20.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.20.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.20.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.20.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.20.mlp.act_fn SiLU()\n",
      "layers.20.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.20.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.21 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.21.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.21.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.21.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.21.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.21.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.21.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.21.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.21.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.21.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.21.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.21.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.21.mlp.act_fn SiLU()\n",
      "layers.21.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.21.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.22 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.22.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.22.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.22.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.22.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.22.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.22.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.22.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.22.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.22.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.22.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.22.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.22.mlp.act_fn SiLU()\n",
      "layers.22.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.22.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.23 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.23.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.23.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.23.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.23.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.23.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.23.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.23.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.23.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.23.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.23.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.23.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.23.mlp.act_fn SiLU()\n",
      "layers.23.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.23.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.24 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.24.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.24.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.24.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.24.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.24.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.24.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.24.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.24.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.24.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.24.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.24.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.24.mlp.act_fn SiLU()\n",
      "layers.24.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.24.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.25 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.25.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.25.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.25.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.25.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.25.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.25.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.25.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.25.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.25.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.25.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.25.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.25.mlp.act_fn SiLU()\n",
      "layers.25.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.25.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.26 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.26.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.26.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.26.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.26.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.26.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.26.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.26.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.26.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.26.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.26.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.26.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.26.mlp.act_fn SiLU()\n",
      "layers.26.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.26.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.27 Qwen3DecoderLayer(\n",
      "  (self_attn): Qwen3Attention(\n",
      "    (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "    (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "    (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  )\n",
      "  (mlp): Qwen3MLP(\n",
      "    (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "    (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "  (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      ")\n",
      "layers.27.self_attn Qwen3Attention(\n",
      "  (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "  (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "  (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      ")\n",
      "layers.27.self_attn.q_proj Linear(in_features=1024, out_features=2048, bias=False)\n",
      "layers.27.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.27.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=False)\n",
      "layers.27.self_attn.o_proj Linear(in_features=2048, out_features=1024, bias=False)\n",
      "layers.27.self_attn.q_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.27.self_attn.k_norm Qwen3RMSNorm((128,), eps=1e-06)\n",
      "layers.27.mlp Qwen3MLP(\n",
      "  (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "  (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "layers.27.mlp.gate_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.27.mlp.up_proj Linear(in_features=1024, out_features=3072, bias=False)\n",
      "layers.27.mlp.down_proj Linear(in_features=3072, out_features=1024, bias=False)\n",
      "layers.27.mlp.act_fn SiLU()\n",
      "layers.27.input_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "layers.27.post_attention_layernorm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "norm Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "rotary_emb Qwen3RotaryEmbedding()\n",
      "embedding shape: torch.Size([8, 1024])\n",
      "[Cluster 0] The capital of China is Beijing.\n",
      "[Cluster 2] The Great Wall is a famous landmark in China.\n",
      "[Cluster 0] Paris is the capital of France.\n",
      "[Cluster 0] The Eiffel Tower is located in Paris.\n",
      "[Cluster 2] Apple releases a new iPhone every year.\n",
      "[Cluster 1] Samsung and Apple are leading smartphone brands.\n",
      "[Cluster 1] Quantum mechanics is a fundamental theory in physics.\n",
      "[Cluster 1] Newton's laws describe classical mechanics.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F \n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def last_token_pool(last_hidden_states, attention_mask):\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        sequence_index = attention_mask.sum(dim = 1) - 1\n",
    "        return last_hidden_states[torch.arange(batch_size, device = last_hidden_states.device), sequence_index]\n",
    "    \n",
    "\n",
    "texts = [\n",
    "    \"The capital of China is Beijing.\",\n",
    "    \"The Great Wall is a famous landmark in China.\",\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"The Eiffel Tower is located in Paris.\",\n",
    "    \"Apple releases a new iPhone every year.\",\n",
    "    \"Samsung and Apple are leading smartphone brands.\",\n",
    "    \"Quantum mechanics is a fundamental theory in physics.\",\n",
    "    \"Newton's laws describe classical mechanics.\",\n",
    "]\n",
    "\n",
    "\n",
    "model_path = '/home/xwj/Model/qwen3-embedding-0.6b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side='left')\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    print(name, module)\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "eod_id = tokenizer.convert_tokens_to_ids(\"<endoftext>\")\n",
    "batch_dict = tokenizer(texts, padding=False, truncation=True, max_length=512-2)\n",
    "for ids, mask in zip(batch_dict[\"input_ids\"], batch_dict[\"attention_mask\"]):\n",
    "    ids.append(eod_id)\n",
    "    mask.append(1) \n",
    "batch_dict = tokenizer.pad(batch_dict, return_tensors=\"pt\")\n",
    "\"\"\"\n",
    "input_texts = [text + \" <|endoftext|>\" for text in texts]\n",
    "batch_dict = tokenizer(\n",
    "    input_texts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=8192,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "batch_dict = {k: v.to(model.device) for k, v in batch_dict.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch_dict)\n",
    "    embeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "print(f\"embedding shape: {embeddings.shape}\")\n",
    "\n",
    "n_clusters = 3\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "labels = kmeans.fit_predict(embeddings.cpu().numpy())\n",
    "\n",
    "\n",
    "for i, (text, label) in enumerate(zip(texts, labels)):\n",
    "    print(f\"[Cluster {label}] {text}\")\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "points_2d = pca.fit_transform(embeddings.cpu().numpy())\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['red', 'blue', 'green', 'purple', 'orange']\n",
    "for i, (point, label) in enumerate(zip(points_2d, labels)):\n",
    "    plt.scatter(point[0], point[1], color=colors[label], label=f\"Cluster {label}\" if f\"Cluster {label}\" not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "    plt.text(point[0]+0.01, point[1], f\"{i}\", fontsize=9)\n",
    "\n",
    "plt.title(\"Text Clustering via Qwen Embedding + KMeans\")\n",
    "plt.xlabel(\"PCA Dim 1\")\n",
    "plt.ylabel(\"PCA Dim 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b046cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xwj_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
