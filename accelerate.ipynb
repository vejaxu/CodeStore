{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b621f4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.2925\n",
      "Epoch 2/5, Loss: 0.1297\n",
      "Epoch 3/5, Loss: 0.0869\n",
      "Epoch 4/5, Loss: 0.0636\n",
      "Epoch 5/5, Loss: 0.0485\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.MNIST(root=\"datasets\", transform=transform, train=True, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(), \n",
    "    nn.Linear(28 * 28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10),\n",
    ")\n",
    "\n",
    "critrtion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)\n",
    "\n",
    "epochs = 5\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for imgs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = critrtion(outputs, targets)\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    epoch_loss /= len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660fe010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/xwj_llm/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 4.6144\n",
      "Epoch 2/3, Loss: 2.0710\n",
      "Epoch 3/3, Loss: 1.9783\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"imdb\", split='train[:500]')\n",
    "\n",
    "model_name = \"/home/xwj/Model/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "encoded_dataset = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=['text'])\n",
    "encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(encoded_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.train()\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 8, \n",
    "    lora_alpha = 32,\n",
    "    target_modules = [\"c_attn\"],\n",
    "    lora_dropout = 0.1,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)\n",
    "\n",
    "epochs = 3\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['input_ids'])\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    epoch_loss /= len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# model.save_pretrained(\"accelerate_gpt2_lora_imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e51b55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xwj_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
